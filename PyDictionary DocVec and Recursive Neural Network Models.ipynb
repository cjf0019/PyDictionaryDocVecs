{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "\n",
    "Word2Vec, an NLP word embedding algorithm of the last five years, creates a vector per word in a corpus vocabulary based on the conditional probability of other \"context\" words appearing next to the target word. Several thousand-dimensioned one-hot vectors, corresponding to an individual word's index in a larger vocabulary, can be condensed into generally 10 - 500 dimensions. Interestingly, these condensed vectors have been shown to capture meaning in their directions. \n",
    "\n",
    "While words, atomic in nature and with straightforward conditional probability calculations, can be straightforwardly translatable into vectors, attempting to vectorize phrases and sentences poses a much greater challenge, due to their compositionality and the grammar rules associated with the more complex meanings that phrases and sentences can convey. Another consideration, the phrase vectors should ideally be commensurable with the word vectors, i.e., must be able to be cast into the same overall vector space, if only through some sort of transformation. \n",
    "\n",
    "The following is an attempt to capture the semantics of phrases and sentences through use of PyDictionary (https://github.com/geekpradd/PyDictionary/tree/master/PyDictionary). A dictionary provides a list of words with equivalent groups of sentences/phrases, i.e., definitions. This list of keys and values could serve as an interesting test case for understanding the effectiveness of strategies for capturing higher meaning. \n",
    "\n",
    "Doc2Vec is one such attempt to vectorize sentences. The algorithm can be performed via two different methods, one of which, \"Distributed Memory,\" mimics the Continuous Bag of Words (CBoW) model of Word2Vec in that it predicts a target word based on either an average or concatenation of context words. In DM, an additional \"paragraph vector\" is added into the average/concatenation. The same vector is applied to each context window in a sentence, with a unique vector for each sentence. So, the vector is aiding in the prediction tasks of words in the sentence.\n",
    "\n",
    "Another method, known as the Recursive Neural Network, can be seen as a generalization of the more well-known Recurrent Neural Network. Recurrent Neural Networks are often used in sequence predictions tasks, in which they assume a linear time. Recursive Neural Networks, on the other hand, break sequences (here sentences) into branches of a tree, performing prediction tasks on all of the branches. Constituency parsing breaks sentences into branches that reflect the grammar of the sentence, which makes RecNN's a good candidate for use in Natural Language Processing. \n",
    "\n",
    "The project is organized as follows: \n",
    "\n",
    "1) An overview of the underlying word2vec model, trained on the Wikipedia Corpus. \n",
    "2) An attempt with a Doc2Vec modification to commensurate the paragraph vectors of definitions with the definitional words' vectors. \n",
    "3) An attempt with Recursive Neural Networks to generate embeddings for phrases and definitions, using cosine similarity with the definitional word as the prediction task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Corpus Word Vector Overview\n",
    "\n",
    "The word vectors and paragraph vectors used in this project come from a child class of Gensim's Doc2Vec (https://radimrehurek.com/gensim/models/doc2vec.html), called \"doc2vecwordfixed.\" This model allows for word vectors to be fixed during paragraph vector training, as to be explained in the next section. First we load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\InfiniteJest\\\\Documents\\\\Python_Scripts')\n",
    "import doc2vecwordfixed\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "wikimodel = doc2vecwordfixed.Doc2VecWordFixed.load('wiki100dmnolbls001samp3000mc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic statistics on the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents:  4240287\n",
      "Total Word Count:  2202030161\n",
      "Top 100 Words\n",
      "['the', 'of', 'and', 'in', 'to', 'was', 'is', 'for', 'as', 'on', 'by', 'with', 'he', 'at', 'that', 'from', 'his', 'it', 'an', 'are', 'were', 'which', 'also', 'this', 'or', 'be', 'first', 'has', 'new', 'had', 'one', 'their', 'not', 'after', 'its', 'who', 'but', 'two', 'her', 'they', 'have', 'she', 'references', 'th', 'all', 'other', 'been', 'time', 'when', 'school', 'during', 'may', 'year', 'into', 'there', 'world', 'city', 'up', 'more', 'no', 'university', 'de', 'state', 'years', 'national', 'united', 'american', 'only', 'over', 'external', 'links', 'most', 'team', 'three', 'film', 'between', 'can', 'would', 'out', 'some', 'later', 'where', 'about', 'used', 'st', 'south', 'states', 'season', 'born', 'such', 'under', 'him', 'then', 'part', 'made', 'second', 'war', 'john', 'known', 'while']\n"
     ]
    }
   ],
   "source": [
    "vocabcount = {}\n",
    "totalwordcount = 0\n",
    "for word in wikimodel.vocab.keys():\n",
    "    wordcount = wikimodel.vocab[word].count\n",
    "    totalwordcount += wordcount\n",
    "    vocabcount[word] = wordcount\n",
    "mostfrequent = sorted(vocabcount, key=lambda key: vocabcount[key], reverse=True)\n",
    "print('Number of Documents: ', wikimodel.corpus_count)\n",
    "print('Total Word Count: ', totalwordcount)\n",
    "print('Top 100 Words')\n",
    "print(mostfrequent[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the top cosine similarity of various words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man:  [('boy', 0.7870639562606812), ('girl', 0.7832261323928833), ('woman', 0.7731713056564331), ('lad', 0.7303745746612549), ('thief', 0.6730341911315918), ('person', 0.6582547426223755), ('swordsman', 0.6563944220542908), ('gambler', 0.6371715664863586), ('gentleman', 0.6117371320724487), ('thug', 0.6019390225410461)]\n",
      "\n",
      "Code:  [('codes', 0.6878383755683899), ('specification', 0.6829293370246887), ('registration', 0.6401716470718384), ('identification', 0.6284632682800293), ('procedure', 0.6190042495727539), ('type', 0.6061720252037048), ('identifier', 0.6001975536346436), ('standard', 0.5941685438156128), ('protocol', 0.5930469036102295), ('prefix', 0.5893802642822266)]\n",
      "\n",
      "Jump:  [('jumper', 0.7318518161773682), ('hurdles', 0.5937206745147705), ('metre', 0.550831139087677), ('metres', 0.5491101741790771), ('jumpers', 0.5344479084014893), ('meter', 0.5268185138702393), ('discus', 0.5235909223556519), ('javelin', 0.5028536915779114), ('throw', 0.498761385679245), ('speed', 0.49796468019485474)]\n",
      "\n",
      "Dirty:  [('nasty', 0.7286748886108398), ('crazy', 0.7160338163375854), ('sexy', 0.7151573896408081), ('funky', 0.7062022686004639), ('dope', 0.6887364387512207), ('sweet', 0.6816124320030212), ('filthy', 0.6815508604049683), ('featuring', 0.6786483526229858), ('kid', 0.6678594350814819), ('mad', 0.661804735660553)]\n",
      "\n",
      "Physics:  [('chemistry', 0.8551139831542969), ('mathematics', 0.8479235768318176), ('biochemistry', 0.8225346803665161), ('astronomy', 0.8124189376831055), ('psychology', 0.7858392596244812), ('astrophysics', 0.7838801145553589), ('microbiology', 0.7761387228965759), ('sociology', 0.7616530656814575), ('biology', 0.7569639682769775), ('geophysics', 0.7556869983673096)]\n",
      "\n",
      "Happy:  [('stupid', 0.7219228744506836), ('lovely', 0.709675133228302), ('foolish', 0.6923747062683105), ('quiet', 0.6870824098587036), ('funny', 0.6862049102783203), ('cute', 0.68439781665802), ('scary', 0.6670947074890137), ('sorry', 0.6605985164642334), ('silly', 0.6547354459762573), ('lonely', 0.6535154581069946)]\n",
      "\n",
      "Betrothed:  [('unbeknownst', 0.7609206438064575), ('unfaithful', 0.7095489501953125), ('oblivious', 0.7069282531738281), ('bequeathed', 0.7067356705665588), ('alluded', 0.6936620473861694), ('confess', 0.6886104345321655), ('ascribed', 0.6751004457473755), ('unwilling', 0.6710450649261475), ('supposed', 0.6693089008331299), ('entrusted', 0.6690890789031982)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Man: \",wikimodel.most_similar('man', topn=10))\n",
    "print()\n",
    "print(\"Code: \", wikimodel.most_similar('code', topn=10))\n",
    "print()\n",
    "print(\"Jump: \", wikimodel.most_similar('jump', topn=10))\n",
    "print()\n",
    "print(\"Dirty: \", wikimodel.most_similar('dirty', topn=10))\n",
    "print()\n",
    "print(\"Physics: \", wikimodel.most_similar('physics', topn=10))\n",
    "print()\n",
    "print(\"Happy: \", wikimodel.most_similar('happy', topn=10))\n",
    "print()\n",
    "print(\"Betrothed: \", wikimodel.most_similar('betrothed', topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the closely aligned vectors make perfect sense, as in Physics with Chemistry, Mathematics, and Biochemistry. It further captures multiple meanings, like Dirty with nasty and sexy. Yet, these vectors are trained, not to represent meaning, but rather the conditional probability of a word appearing next to other words. So, they are really showing what words have similar contexts and can sometimes miss underlying semantics. For example, Betrothed matches closest with unbeknownst and unfaithful, which are actually contradictory to the meaning. \n",
    "\n",
    "Next, we can look at visualizing vectors. To do that, we must first reduce dimensionality from 100 to 2. We accomplish this using the tSNE algorithm, part of the sci-kit learn package (http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE()\n",
    "Y = tsne.fit_transform(wikimodel.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickwords(model1, wordvectransform, testwords):   #wordvectransform is the tsne vectors\n",
    "    testvocabvectors = []\n",
    "    for word in testwords:\n",
    "        testvocabvectors.append(model1[word])  #retrieves the vector of each word from the model\n",
    "    modelindices = []\n",
    "    for i in testwords:\n",
    "        modelindices.append(list(model1.vocab).index(i))\n",
    "    mag = 3*len(testwords) \n",
    "    plt.scatter(mag*wordvectransform[modelindices[:], 0], mag*wordvectransform[modelindices[:], 1])\n",
    "    for label, x, y in zip(testwords, mag*wordvectransform[modelindices[:], 0], mag*wordvectransform[modelindices[:], 1]):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a set of words and retrieve their tSNE-transformed vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testwords = ['up', 'down', 'man', 'woman', 'king', 'queen', 'happy', 'sad', 'emotions', 'car', 'drive', 'bike', 'ride']\n",
    "\n",
    "testvocabvectors = []\n",
    "for word in testwords:\n",
    "    testvocabvectors.append(wikimodel[word])  #retrieves the vector of each word from the model\n",
    "modelindices = []\n",
    "for i in testwords:\n",
    "    modelindices.append(list(wikimodel.vocab).index(i))\n",
    "mag = 3*len(testwords) \n",
    "#plt.scatter(mag*wordvectransform[modelindices[:], 0], mag*wordvectransform[modelindices[:], 1])\n",
    "#for label, x, y in zip(testwords, mag*wordvectransform[modelindices[:], 0], mag*wordvectransform[modelindices[:], 1]):\n",
    "#    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"32e64c7a-e8fc-4da2-b15c-5ecbe39dab63\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      var el = document.getElementById(\"32e64c7a-e8fc-4da2-b15c-5ecbe39dab63\");\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"32e64c7a-e8fc-4da2-b15c-5ecbe39dab63\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '32e64c7a-e8fc-4da2-b15c-5ecbe39dab63' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.7.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"32e64c7a-e8fc-4da2-b15c-5ecbe39dab63\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"32e64c7a-e8fc-4da2-b15c-5ecbe39dab63\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0fca99c74001>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwikimodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwikimodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodelindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mradii\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodelindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m   1504\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1506\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1508\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1504\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1506\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1508\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "import numpy as np\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "N = len(wikimodel.vocab.keys())\n",
    "x = wikimodel[modelindices[:], 0] * 100\n",
    "y = np.random.random(size=N) * 100\n",
    "radii = Y[modelindices[:]] * 1.5\n",
    "colors = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), 150) for r, g in zip(50+2*x, 30+2*y)\n",
    "]\n",
    "\n",
    "TOOLS=\"crosshair,pan,wheel_zoom,box_zoom,reset,box_select,lasso_select\"\n",
    "\n",
    "# create a new plot with the tools above, and explicit ranges\n",
    "p = figure(tools=TOOLS, x_range=(-100,100), y_range=(-100,100))\n",
    "\n",
    "# add a circle renderer with vectorized colors and sizes\n",
    "p.circle(x,y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\n",
    "\n",
    "# show the results\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "firstcomponent = pca.components_[0]\n",
    "model.similar_by_vector(firstcomponent)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "onecomponent = pca2.fit_transform(model.syn0)\n",
    "\n",
    "def getmodelindices(model, wordlist, docindices=False):\n",
    "    indices = []\n",
    "    if docindices == True:\n",
    "        for word in wordlist:\n",
    "            indices.append(list(model.docvecs.doctags).index(word))\n",
    "    else:\n",
    "        for word in wordlist:\n",
    "            indices.append(list(model.vocab.keys()).index(word))\n",
    "    return indices\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "def concatenatedocandwordvectors(model, vector_size):\n",
    "    combinedsyn0 = []\n",
    "    combinedwordlist = []\n",
    "    for word in list(model.vocab.keys()):\n",
    "        if word in list(model.docvecs.doctags):\n",
    "            docindex = list(model.docvecs.doctags).index(word)\n",
    "            combinedsyn0.append(np.concatenate([model[word],model.docvecs[docindex]]))\n",
    "    combinedsyn0 = np.vstack(combinedsyn0)\n",
    "    for i in combinedsyn0:\n",
    "        for j in range(len(model.syn0)):\n",
    "            if np.array_equal(i[0:vector_size], model.syn0[j]):\n",
    "                combinedwordlist.append(model.index2word[j])\n",
    "    return combinedsyn0, combinedwordlist\n",
    "    \n",
    "finalvecs, wordlist = concatenatedocandwordvectors(model, 100)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE()\n",
    "tsne2 = TSNE()\n",
    "Y = tsne.fit_transform(model.syn0)\n",
    "#Y2 = tsne.fit_transform(finalvecs)\n",
    "Y2 = tsne2.fit_transform(model.docvecs)\n",
    "\n",
    "def pickwords(model1, docvectransform, wordvectransform):\n",
    "    words = set(model1.docvecs.doctags.keys())\n",
    "    testinput = input(\"Type a list of words you would like to compare:\")\n",
    "    testwords = testinput.split(\" \")\n",
    "    for word in testwords:\n",
    "        if word not in words:\n",
    "            testwords.pop(testwords.index(word))\n",
    "    testvocabvectors = []\n",
    "    testdocvectors = []\n",
    "    for word in testwords:\n",
    "        testvocabvectors.append(model1[word])\n",
    "        docindex = list(model1.docvecs.doctags).index(word)\n",
    "        testdocvectors.append(model1.docvecs[docindex])\n",
    "    \n",
    "    modelindices = []\n",
    "    modeldocindices = []\n",
    "    for i in testwords:\n",
    "        modelindices.append(list(model1.vocab).index(i))\n",
    "        modeldocindices.append(list(model1.docvecs.doctags).index(i))\n",
    "    mag = 3*len(testwords)\n",
    "    import matplotlib.pyplot as plt    \n",
    "    plt.scatter(mag*wordvectransform[modelindices[:], 0], mag*wordvectransform[modelindices[:], 1])\n",
    "    for label, x, y in zip(testwords, mag*wordvectransform[modelindices[:], 0], mag*wordvectransform[modelindices[:], 1]):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(mag*docvectransform[modeldocindices[:], 0], mag*docvectransform[modeldocindices[:], 1])\n",
    "    for label, x, y in zip(testwords, mag*docvectransform[modeldocindices[:], 0], mag*docvectransform[modeldocindices[:], 1]):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "#testwords = list(model.docvecs.doctags.keys())\n",
    "\n",
    "testwords = ['king', 'queen', 'man', 'woman', 'poor', 'rich', 'garbage', 'beautiful', 'argue', 'love', 'hate', 'life', 'bother', 'exam', 'human', 'animal', 'hint', 'fear', 'anxiety', 'lose', 'win', 'dog', 'cat', 'bird', 'mouse', 'big', 'large', 'boring', 'war', 'weapon', 'peace', 'prosperity']\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "mag = 1000000\n",
    "combinedmodelindices = []\n",
    "controlindices = []\n",
    "\n",
    "\n",
    "####Do if not concatenating the wordvecs\n",
    "if \"wordlist\" not in globals():\n",
    "    wordlist = list(set(list(model.docvecs.doctags)).intersection(model.vocab.keys()))\n",
    "docvecindices = getmodelindices(model, wordlist, docindices=True)\n",
    "wordvecindices = getmodelindices(model, wordlist)\n",
    "\n",
    "testwordindices = []\n",
    "testdocindices = []\n",
    "for word in testwords:\n",
    "    if word in set(wordlist):\n",
    "        testwordindices.append(wordvecindices[wordlist.index(word)])\n",
    "        testdocindices.append(docvecindices[wordlist.index(word)])\n",
    "        \n",
    "\n",
    "plt.scatter(mag*Y[testwordindices[:], 0], mag*Y[testwordindices[:], 1])\n",
    "for label, x, y in zip(testwords, mag*Y[testwordindices[:], 0], mag*Y[testwordindices[:], 1]):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(mag*Y2[testdocindices[:], 0], mag*Y2[testdocindices[:], 1])\n",
    "for label, x, y in zip(testwords, mag*Y2[testdocindices[:], 0], mag*Y2[testdocindices[:], 1]):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()\n",
    "####\n",
    "    \n",
    "    \n",
    "for word in testwords:\n",
    "    if word in set(wordlist):\n",
    "        combinedmodelindices.append(wordlist.index(word))\n",
    "    controlindices.append(list(model.vocab.keys()).index(word))\n",
    "plt.scatter(mag*Y[controlindices[:], 0], mag*Y[controlindices[:], 1])\n",
    "for label, x, y in zip(testwords, mag*Y[controlindices[:], 0], mag*Y[controlindices[:], 1]):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(mag*Y2[combinedmodelindices[:], 0], mag*Y2[combinedmodelindices[:], 1])\n",
    "for label, x, y in zip(testwords, mag*Y2[combinedmodelindices[:], 0], mag*Y2[combinedmodelindices[:], 1]):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deriving Embeddings from a PyDictionary Recursive Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recursive Neural Networks have primarily been used in NLP for parsing and sentiment analysis. The usefulness of them resides in their ability to break sentences down into parts that can be individually trained and assigned a vector in the same space as the word vectors. Unlike word2vec, which trains the vector space through the prediction conditional probility distributions with other words, RecNN's have traditionally used sentiment labels, such as a number 0 through 5 indicating how positive or negative a word or phrase is, or parsing labels. \n",
    "\n",
    "For the current PyDictionary corpus, the training process is modified in the following ways:\n",
    "1) The word2vec embeddings trained from the Wikipedia corpus are used and kept fixed. \n",
    "2) The prediction task is to predict the definitional word from its parsed definition. That is, the labels are the word2vec embeddings of the definitional word. \n",
    "3) The objective is now to maximize the dot product of the proposed word/phrase/sentence vectors with the label vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GIVE AN EXAMPLE!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
